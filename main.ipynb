{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5094410,"sourceType":"datasetVersion","datasetId":2958426}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/fine-tuning-a-gpt-2-model-for-a-custom-chatbot?scriptVersionId=265217942\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"background-color: #013220; color: #EAEAEA; border-radius: 15px; padding: 30px; font-family: 'Helvetica Neue', sans-serif; border: 1px solid #388E3C;\">\n\n<div style=\"text-align: center; font-size: 32px; font-weight: bold; color: #A5D6A7; padding-bottom: 15px; border-bottom: 2px solid #388E3C; margin-bottom: 25px;\">\n        Fine-Tuning a GPT-2 Model for a Custom Chatbot\n    </div>\n\n<div style=\"font-size: 20px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Project Overview\n    </div>\n    <div style=\"font-size: 16px; line-height: 1.7; text-align: justify; margin-bottom: 20px;\">\n        Welcome to this hands-on guide to fine-tuning a Large Language Model (LLM)! In this project, we will transform a general-purpose language model, the powerful GPT-2, into a specialized chatbot. Our goal is to leverage a dataset of 3,000 conversational pairs to teach the model a specific conversational style and knowledge base. This process, known as fine-tuning, is a cornerstone of modern NLP, allowing us to adapt massive pre-trained models for specific tasks without the prohibitive cost of training them from scratch. By the end of this notebook, you will have a functional chatbot customized with our data and a clear understanding of the end-to-end LLM fine-tuning workflow.\n    </div>\n\n<div style=\"font-size: 20px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        The Core Objective\n    </div>\n    <div style=\"font-size: 16px; line-height: 1.7; text-align: justify; margin-bottom: 20px;\">\n        Our primary objective is to demonstrate the power of transfer learning in NLP. We will take the `distilgpt2` model—a smaller, faster variant of GPT-2 that is perfect for experimentation in environments like Kaggle—and train it further on a custom question-and-answer dataset. We will then compare the responses of the original, base model with our newly fine-tuned model to concretely measure the improvement and see how the model's personality and knowledge have shifted.\n    </div>\n\n <div style=\"font-size: 20px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Methodology at a Glance\n    </div>\n    <div style=\"font-size: 16px; line-height: 1.7; text-align: justify;\">\n        Our approach is structured into a clear, step-by-step process:\n        <div style=\"margin-top: 15px; padding-left: 20px;\">\n            <div style=\"margin-bottom: 10px;\"><b>1. Environment Setup:</b> We will import essential libraries like PyTorch and Hugging Face's `transformers` and `datasets`.</div>\n            <div style=\"margin-bottom: 10px;\"><b>2. Data Loading & Preparation:</b> We will load the 3k conversations dataset and format it into a single text sequence per conversation, making it suitable for training a causal language model like GPT-2.</div>\n            <div style=\"margin-bottom: 10px;\"><b>3. Model & Tokenizer Initialization:</b> We will load the pre-trained `distilgpt2` model and its corresponding tokenizer, setting up the necessary configurations for training.</div>\n            <div style=\"margin-bottom: 10px;\"><b>4. Fine-Tuning:</b> Using the Hugging Face `Trainer` API, we will fine-tune the model on our prepared dataset. This is where the model learns the new conversational patterns.</div>\n            <div style=\"margin-bottom: 10px;\"><b>5. Inference & Evaluation:</b> Finally, we'll test our new chatbot! We will provide it with prompts and compare its generated answers against those from the original model to see our improvements in action.</div>\n        </div>\n    </div>\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #013220; color: #EAEAEA; border-radius: 15px; padding: 30px; font-family: 'Helvetica Neue', sans-serif; border: 1px solid #388E3C;\">\n\n <div style=\"text-align: center; font-size: 28px; font-weight: bold; color: #A5D6A7; padding-bottom: 15px; border-bottom: 2px solid #388E3C; margin-bottom: 25px;\">\n        Step 2: Environment Setup & Loading the Base Model/Tokenizer\n    </div>\n\n <div style=\"font-size: 18px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Objective of This Step\n    </div>\n    <div style=\"font-size: 15px; line-height: 1.7; text-align: justify; margin-bottom: 20px;\">\n        In this step, we prepare our Kaggle environment for working with Large Language Models using the Hugging Face <code>transformers</code> library alongside <code>PyTorch</code>. This includes installing any required dependencies, importing essential packages, and initializing the building blocks of our project — the pre-trained base model and its tokenizer.\n        The tokenizer is responsible for converting human-readable text into a sequence of tokens (numerical IDs) that the model can understand, while the model itself is the neural network that processes those tokens to generate text predictions.\n    </div>\n\n<div style=\"font-size: 18px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Why We Use DistilGPT-2\n    </div>\n    <div style=\"font-size: 15px; line-height: 1.7; text-align: justify; margin-bottom: 20px;\">\n        We will use <code>distilgpt2</code>, which is a distilled (smaller and faster) version of GPT-2. This choice allows us to train and experiment quickly in Kaggle's GPU environment without exhausting memory or exceeding time limits, while still benefiting from the rich language understanding learned during GPT-2's original training.\n    </div>\n\n<div style=\"font-size: 18px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Actions We Will Perform\n    </div>\n    <div style=\"font-size: 15px; line-height: 1.7; text-align: justify;\">\n        <div style=\"padding-left: 20px;\">\n            <div style=\"margin-bottom: 10px;\"><b>1.</b> Import the necessary Python packages: <code>transformers</code>, <code>torch</code>, and optionally <code>datasets</code>.</div>\n            <div style=\"margin-bottom: 10px;\"><b>2.</b> Initialize the model <code>distilgpt2</code> using the Hugging Face model hub.</div>\n            <div style=\"margin-bottom: 10px;\"><b>3.</b> Load the corresponding tokenizer so we can transform raw text into token IDs for the model to process.</div>\n            <div style=\"margin-bottom: 10px;\"><b>4.</b> Verify that the model and tokenizer are ready for fine-tuning on our custom dataset in the next steps.</div>\n        </div>\n    </div>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"\n\n# Install the Hugging Face Transformers library (if not already available in Kaggle)\n#!pip install transformers datasets --quiet\n\n# Import required libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the tokenizer for distilgpt2\ntokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n\n# Load the base model (DistilGPT-2) for Causal Language Modeling\nmodel = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n\n# Verify model and tokenizer are ready\nsample_text = \"Hello, how are you?\"\nencoded_input = tokenizer.encode(sample_text, return_tensors=\"pt\").to(device)\nprint(\"Sample token IDs:\", encoded_input)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T07:59:16.790546Z","iopub.execute_input":"2025-10-02T07:59:16.790789Z","iopub.status.idle":"2025-10-02T07:59:17.227446Z","shell.execute_reply.started":"2025-10-02T07:59:16.790767Z","shell.execute_reply":"2025-10-02T07:59:17.226586Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nSample token IDs: tensor([[15496,    11,   703,   389,   345,    30]])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\n# Dataset path in Kaggle\ndataset_path = \"/kaggle/input/3k-conversations-dataset-for-chatbot/Conversation.csv\"\n\n# Load CSV file into a Pandas DataFrame\ndf = pd.read_csv(dataset_path)\n\n# Show basic info about the dataset\nprint(\"Dataset Shape:\", df.shape)\nprint(\"\\nColumn Names:\", df.columns.tolist())\n\n# Display first few rows\nprint(\"\\nSample Rows:\")\nprint(df.head())\n\n# Check for missing values\nprint(\"\\nMissing Values Per Column:\")\nprint(df.isnull().sum())\n\n# Show basic statistics (if numerical columns exist)\nprint(\"\\nDataset Statistics:\")\nprint(df.describe(include='all'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T08:01:25.205817Z","iopub.execute_input":"2025-10-02T08:01:25.206135Z","iopub.status.idle":"2025-10-02T08:01:25.268244Z","shell.execute_reply.started":"2025-10-02T08:01:25.206092Z","shell.execute_reply":"2025-10-02T08:01:25.267404Z"}},"outputs":[{"name":"stdout","text":"Dataset Shape: (3725, 3)\n\nColumn Names: ['Unnamed: 0', 'question', 'answer']\n\nSample Rows:\n   Unnamed: 0                             question  \\\n0           0               hi, how are you doing?   \n1           1        i'm fine. how about yourself?   \n2           2  i'm pretty good. thanks for asking.   \n3           3    no problem. so how have you been?   \n4           4     i've been great. what about you?   \n\n                                     answer  \n0             i'm fine. how about yourself?  \n1       i'm pretty good. thanks for asking.  \n2         no problem. so how have you been?  \n3          i've been great. what about you?  \n4  i've been good. i'm in school right now.  \n\nMissing Values Per Column:\nUnnamed: 0    0\nquestion      0\nanswer        0\ndtype: int64\n\nDataset Statistics:\n         Unnamed: 0           question             answer\ncount   3725.000000               3725               3725\nunique          NaN               3510               3512\ntop             NaN  what do you mean?  what do you mean?\nfreq            NaN                 22                 22\nmean    1862.000000                NaN                NaN\nstd     1075.459204                NaN                NaN\nmin        0.000000                NaN                NaN\n25%      931.000000                NaN                NaN\n50%     1862.000000                NaN                NaN\n75%     2793.000000                NaN                NaN\nmax     3724.000000                NaN                NaN\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"<div style=\"background-color: #013220; color: #EAEAEA; border-radius: 15px; padding: 30px; font-family: 'Helvetica Neue', sans-serif; border: 1px solid #388E3C;\">\n\n<div style=\"text-align: center; font-size: 28px; font-weight: bold; color: #A5D6A7; padding-bottom: 15px; border-bottom: 2px solid #388E3C; margin-bottom: 25px;\">\n        Step 3: Data Preparation for Fine-Tuning\n    </div>\n\n<div style=\"font-size: 18px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Objective of This Step\n    </div>\n    <div style=\"font-size: 15px; line-height: 1.7; text-align: justify; margin-bottom: 20px;\">\n        The core objective here is to transform our structured dataset (with 'question' and 'answer' columns) into a format suitable for training a causal language model. GPT-2 learns by predicting the next token in a continuous sequence of text. Our current two-column format isn't a continuous sequence, so we must combine them into one. We will create a single text string for each row that clearly represents the conversational flow.\n    </div>\n\n<div style=\"font-size: 18px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Our Formatting Strategy\n    </div>\n    <div style=\"font-size: 15px; line-height: 1.7; text-align: justify; margin-bottom: 20px;\">\n        To help the model learn the structure of a conversation, we will format each row into a single string like this:\n        <br><br>\n        <code>\"&lt;s&gt;[Q]: How are you? [A]: I'm doing great, thanks!&lt;/s&gt;\"</code>\n        <br><br>\n        Here, <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> represent the <b>start</b> and <b>end</b> of a single conversational exchange. This special token (known as the EOS or End-of-Sequence token) is crucial because it teaches the model when a thought or response is complete, which is vital for generating coherent answers during inference.\n    </div>\n\n<div style=\"font-size: 18px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Actions We Will Perform\n    </div>\n    <div style=\"font-size: 15px; line-height: 1.7; text-align: justify;\">\n        <div style=\"padding-left: 20px;\">\n            <div style=\"margin-bottom: 10px;\"><b>1. Combine Columns:</b> We will merge the 'question' and 'answer' columns into a new column, applying our formatting rule.</div>\n            <div style=\"margin-bottom: 10px;\"><b>2. Configure Tokenizer:</b> Since <code>distilgpt2</code> does not have a default padding token, we will set its padding token to be the same as its end-of-sequence (EOS) token. This is a standard practice for this model.</div>\n            <div style=\"margin-bottom: 10px;\"><b>3. Create a Dataset Object:</b> We will convert our Pandas DataFrame into a Hugging Face <code>Dataset</code> object for efficient processing and tokenization.</div>\n        </div>\n    </div>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Step 3: Data Preparation for Fine-Tuning\n\n\n\nfrom datasets import Dataset\n\n# --- 1. Format the data into a single string per row ---\n# We will format our data as \"<s>[Q]: {question} [A]: {answer}</s>\"\n# <s> and </s> are special tokens that mark the start and end of a sequence.\n# This helps the model understand the structure of a complete conversational turn.\n\ndef format_conversation(row):\n    return f\"<s>[Q]: {row['question']} [A]: {row['answer']}</s>\"\n\n# Apply the formatting to the DataFrame\ndf['text'] = df.apply(format_conversation, axis=1)\n\nprint(\"Sample of formatted data:\")\nprint(df['text'].head())\n\n\n# --- 2. Configure the tokenizer ---\n# The distilgpt2 model doesn't have a default padding token.\n# We'll use the end-of-sequence (EOS) token as our padding token.\n# This is a common practice for GPT-style models.\ntokenizer.pad_token = tokenizer.eos_token\n\n\n# --- 3. Create a Hugging Face Dataset object ---\n# The Hugging Face Trainer API works best with 'Dataset' objects.\n# We will convert our formatted text column from the pandas DataFrame\n# into this specialized format.\n\n# First, create a new DataFrame with only the text we need\ntraining_data = pd.DataFrame({'text': df['text']})\n\n# Convert the pandas DataFrame to a Hugging Face Dataset\ndataset = Dataset.from_pandas(training_data)\n\n# --- 4. Tokenize the entire dataset ---\n# We'll apply the tokenizer to all our text examples. The tokenizer will\n# convert the text into numerical IDs that the model can understand.\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n\nprint(\"\\nDataset has been processed and tokenized.\")\nprint(\"A single example from the tokenized dataset:\")\nprint(tokenized_dataset[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T08:23:39.746355Z","iopub.execute_input":"2025-10-02T08:23:39.746647Z","iopub.status.idle":"2025-10-02T08:23:41.439989Z","shell.execute_reply.started":"2025-10-02T08:23:39.746621Z","shell.execute_reply":"2025-10-02T08:23:41.439021Z"}},"outputs":[{"name":"stdout","text":"Sample of formatted data:\n0    <s>[Q]: hi, how are you doing? [A]: i'm fine. ...\n1    <s>[Q]: i'm fine. how about yourself? [A]: i'm...\n2    <s>[Q]: i'm pretty good. thanks for asking. [A...\n3    <s>[Q]: no problem. so how have you been? [A]:...\n4    <s>[Q]: i've been great. what about you? [A]: ...\nName: text, dtype: object\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25ae2c2a30ef40dcbb81e089659e5338"}},"metadata":{}},{"name":"stdout","text":"\nDataset has been processed and tokenized.\nA single example from the tokenized dataset:\n{'input_ids': [27, 82, 36937, 48, 5974, 23105, 11, 703, 389, 345, 1804, 30, 685, 32, 5974, 1312, 1101, 3734, 13, 703, 546, 3511, 30, 3556, 82, 29, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"<div style=\"background-color: #013220; color: #EAEAEA; border-radius: 15px; padding: 30px; font-family: 'Helvetica Neue', sans-serif; border: 1px solid #388E3C;\">\n\n <div style=\"text-align: center; font-size: 28px; font-weight: bold; color: #A5D6A7; padding-bottom: 15px; border-bottom: 2px solid #388E3C; margin-bottom: 25px;\">\n        Step 4: Fine-Tuning the Language Model\n    </div>\n\n<div style=\"font-size: 18px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Objective of This Step\n    </div>\n    <div style=\"font-size: 15px; line-height: 1.7; text-align: justify; margin-bottom: 20px;\">\n        This is the core of our project. We will now take the pre-trained <code>distilgpt2</code> model and train it further on our specially prepared conversational dataset. This process, known as fine-tuning, adjusts the model's internal weights to specialize its knowledge and conversational style to match our data. To manage the complexities of a training loop (like batching data, calculating loss, and updating model weights), we will use the high-level <code>Trainer</code> API from the Hugging Face library, which simplifies the entire process immensely.\n    </div>\n\n<div style=\"font-size: 18px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Key Components: `TrainingArguments` and `Trainer`\n    </div>\n    <div style=\"font-size: 15px; line-height: 1.7; text-align: justify; margin-bottom: 20px;\">\n        The <code>Trainer</code> API requires two main components:\n        <br><br>\n        1. <b><code>TrainingArguments</code></b>: This is a configuration class where we define all the hyperparameters for our training run. This includes the number of epochs (how many times to go through the data), the batch size, the learning rate, and where to save the trained model.\n        <br><br>\n        2. <b><code>Trainer</code></b>: This is the main class that orchestrates the training. We simply provide it with our model, the training arguments, and our tokenized dataset. It handles the entire training loop automatically.\n    </div>\n\n<div style=\"font-size: 18px; font-weight: bold; color: #81C784; margin-top: 25px; margin-bottom: 10px;\">\n        Actions We Will Perform\n    </div>\n    <div style=\"font-size: 15px; line-height: 1.7; text-align: justify;\">\n        <div style=\"padding-left: 20px;\">\n            <div style=\"margin-bottom: 10px;\"><b>1. Define Training Arguments:</b> We'll set up the training configuration, choosing parameters that are well-suited for a Kaggle environment.</div>\n            <div style=\"margin-bottom: 10px;\"><b>2. Instantiate the Trainer:</b> We will create an instance of the <code>Trainer</code>, passing it our model, dataset, and arguments.</div>\n            <div style=\"margin-bottom: 10px;\"><b>3. Launch Training:</b> We will call the <code>.train()</code> method to start the fine-tuning process. You will see the training loss decrease over time, indicating that the model is learning.</div>\n            <div style=\"margin-bottom: 10px;\"><b>4. Save the Final Model:</b> Once training is complete, we will save our newly fine-tuned model to a directory so we can use it for inference in the next step.</div>\n        </div>\n    </div>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# Step 4: Fine-Tuning the Model (Corrected Code)\n# ===============================================\n\nfrom transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\n# --- 1. Define Training Arguments ---\n# (This part remains exactly the same)\ntraining_args = TrainingArguments(\n    output_dir=\"./chatbot_model\",\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    warmup_steps=100,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=500,\n    save_strategy=\"epoch\",\n    report_to=\"none\"\n)\n\n# --- 2. Create the Data Collator ---\n# This collator will automatically create the 'labels' needed for the loss calculation.\n# For GPT-2 style models (Causal LM), we set mlm=False.\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, \n    mlm=False\n)\n\n# --- 3. Instantiate the Trainer ---\n# We now add the data_collator to the Trainer.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,  # <-- This is the new key argument\n)\n\n# --- 4. Launch Training ---\nprint(\"Starting the fine-tuning process...\")\ntrainer.train()\nprint(\"Fine-tuning complete.\")\n\n# --- 5. Save the Final Model and Tokenizer ---\ntrainer.save_model(\"./chatbot_model\")\ntokenizer.save_pretrained(\"./chatbot_model\")\n\nprint(\"\\nModel and tokenizer have been saved to './chatbot_model'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T08:33:10.205031Z","iopub.execute_input":"2025-10-02T08:33:10.205365Z"}},"outputs":[{"name":"stdout","text":"Starting the fine-tuning process...\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1417' max='1863' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1417/1863 43:26 < 13:41, 0.54 it/s, Epoch 0.76/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.239200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.937000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null}]}